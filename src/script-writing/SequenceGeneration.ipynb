{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ratsnlp.nlpbook.generation import GenerationTrainArguments\n",
    "args = GenerationTrainArguments(\n",
    "    pretrained_model_name='skt/kogpt2-base-v2',\n",
    "    downstream_corpus_name='nsmc',\n",
    "    downstream_model_dir='./ckpt',\n",
    "    downstream_corpus_root_dir='./korpora',\n",
    "    max_seq_length=32,\n",
    "    batch_size=32 if torch.cuda.is_available() else 4,\n",
    "    learning_rate=5e-5,\n",
    "    epochs=3,\n",
    "    tpu_cores=0 if torch.cuda.is_available() else 8,\n",
    "    seed=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set seed: 7\n"
     ]
    }
   ],
   "source": [
    "from ratsnlp import nlpbook\n",
    "nlpbook.set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ratsnlp:Training/evaluation parameters GenerationTrainArguments(pretrained_model_name='skt/kogpt2-base-v2', downstream_task_name='sentence-generation', downstream_corpus_name='nsmc', downstream_corpus_root_dir='./korpora', downstream_model_dir='./ckpt', max_seq_length=32, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, cpu_workers=8, fp16=False, tpu_cores=0)\n",
      "INFO:ratsnlp:Training/evaluation parameters GenerationTrainArguments(pretrained_model_name='skt/kogpt2-base-v2', downstream_task_name='sentence-generation', downstream_corpus_name='nsmc', downstream_corpus_root_dir='./korpora', downstream_model_dir='./ckpt', max_seq_length=32, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, cpu_workers=8, fp16=False, tpu_cores=0)\n",
      "INFO:ratsnlp:Training/evaluation parameters GenerationTrainArguments(pretrained_model_name='skt/kogpt2-base-v2', downstream_task_name='sentence-generation', downstream_corpus_name='nsmc', downstream_corpus_root_dir='./korpora', downstream_model_dir='./ckpt', max_seq_length=32, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, cpu_workers=8, fp16=False, tpu_cores=0)\n"
     ]
    }
   ],
   "source": [
    "nlpbook.set_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nsmc] download ratings_train.txt: 14.6MB [00:00, 51.3MB/s]                            \n",
      "[nsmc] download ratings_test.txt: 4.90MB [00:00, 39.3MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "Korpora.fetch(\n",
    "    corpus_name=args.downstream_corpus_name,\n",
    "    root_dir=args.downstream_corpus_root_dir,\n",
    "    force_download=args.force_download,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    eos_token='</s>',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ratsnlp:Creating features from dataset file at ./korpora/nsmc\n",
      "INFO:ratsnlp:Creating features from dataset file at ./korpora/nsmc\n",
      "INFO:ratsnlp:Creating features from dataset file at ./korpora/nsmc\n",
      "INFO:ratsnlp:loading train data... LOOKING AT ./korpora/nsmc/ratings_train.txt\n",
      "INFO:ratsnlp:loading train data... LOOKING AT ./korpora/nsmc/ratings_train.txt\n",
      "INFO:ratsnlp:loading train data... LOOKING AT ./korpora/nsmc/ratings_train.txt\n",
      "INFO:ratsnlp:tokenize sentences, it could take a lot of time...\n",
      "INFO:ratsnlp:tokenize sentences, it could take a lot of time...\n",
      "INFO:ratsnlp:tokenize sentences, it could take a lot of time...\n",
      "INFO:ratsnlp:tokenize sentences [took 2.410 s]\n",
      "INFO:ratsnlp:tokenize sentences [took 2.410 s]\n",
      "INFO:ratsnlp:tokenize sentences [took 2.410 s]\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:sentence: 부정 아 더빙.. 진짜 짜증나네요 목소리\n",
      "INFO:ratsnlp:sentence: 부정 아 더빙.. 진짜 짜증나네요 목소리\n",
      "INFO:ratsnlp:sentence: 부정 아 더빙.. 진짜 짜증나네요 목소리\n",
      "INFO:ratsnlp:tokens: ▁부정 ▁아 ▁더 빙 .. ▁진짜 ▁짜 증 나 네 요 ▁목소리 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:tokens: ▁부정 ▁아 ▁더 빙 .. ▁진짜 ▁짜 증 나 네 요 ▁목소리 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:tokens: ▁부정 ▁아 ▁더 빙 .. ▁진짜 ▁짜 증 나 네 요 ▁목소리 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[11775, 9050, 9267, 7700, 9705, 23971, 12870, 8262, 7055, 7098, 8084, 48213, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[11775, 9050, 9267, 7700, 9705, 23971, 12870, 8262, 7055, 7098, 8084, 48213, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[11775, 9050, 9267, 7700, 9705, 23971, 12870, 8262, 7055, 7098, 8084, 48213, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[11775, 9050, 9267, 7700, 9705, 23971, 12870, 8262, 7055, 7098, 8084, 48213, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[11775, 9050, 9267, 7700, 9705, 23971, 12870, 8262, 7055, 7098, 8084, 48213, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[11775, 9050, 9267, 7700, 9705, 23971, 12870, 8262, 7055, 7098, 8084, 48213, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:sentence: 긍정 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "INFO:ratsnlp:sentence: 긍정 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "INFO:ratsnlp:sentence: 긍정 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "INFO:ratsnlp:tokens: ▁긍정 ▁흠 ... 포 스터 보고 ▁초 딩 영화 줄 .... 오 버 연기 조차 ▁가볍 지 ▁않 구나 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:tokens: ▁긍정 ▁흠 ... 포 스터 보고 ▁초 딩 영화 줄 .... 오 버 연기 조차 ▁가볍 지 ▁않 구나 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:tokens: ▁긍정 ▁흠 ... 포 스터 보고 ▁초 딩 영화 줄 .... 오 버 연기 조차 ▁가볍 지 ▁않 구나 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[16420, 19243, 29045, 8658, 11211, 11213, 9206, 7301, 14558, 8239, 10765, 8052, 7621, 31542, 15651, 20364, 8263, 9111, 12226, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[16420, 19243, 29045, 8658, 11211, 11213, 9206, 7301, 14558, 8239, 10765, 8052, 7621, 31542, 15651, 20364, 8263, 9111, 12226, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[16420, 19243, 29045, 8658, 11211, 11213, 9206, 7301, 14558, 8239, 10765, 8052, 7621, 31542, 15651, 20364, 8263, 9111, 12226, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[16420, 19243, 29045, 8658, 11211, 11213, 9206, 7301, 14558, 8239, 10765, 8052, 7621, 31542, 15651, 20364, 8263, 9111, 12226, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[16420, 19243, 29045, 8658, 11211, 11213, 9206, 7301, 14558, 8239, 10765, 8052, 7621, 31542, 15651, 20364, 8263, 9111, 12226, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[16420, 19243, 29045, 8658, 11211, 11213, 9206, 7301, 14558, 8239, 10765, 8052, 7621, 31542, 15651, 20364, 8263, 9111, 12226, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:sentence: 부정 너무재밓었다그래서보는것을추천한다\n",
      "INFO:ratsnlp:sentence: 부정 너무재밓었다그래서보는것을추천한다\n",
      "INFO:ratsnlp:sentence: 부정 너무재밓었다그래서보는것을추천한다\n",
      "INFO:ratsnlp:tokens: ▁부정 ▁너무 재 <unk> 었 다 그래 서 보는 것 을 추천 한다 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:tokens: ▁부정 ▁너무 재 <unk> 었 다 그래 서 보는 것 을 추천 한다 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:tokens: ▁부정 ▁너무 재 <unk> 었 다 그래 서 보는 것 을 추천 한다 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[11775, 12371, 8170, 5, 8017, 7182, 19561, 7788, 12399, 6860, 8137, 32217, 10013, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[11775, 12371, 8170, 5, 8017, 7182, 19561, 7788, 12399, 6860, 8137, 32217, 10013, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[11775, 12371, 8170, 5, 8017, 7182, 19561, 7788, 12399, 6860, 8137, 32217, 10013, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[11775, 12371, 8170, 5, 8017, 7182, 19561, 7788, 12399, 6860, 8137, 32217, 10013, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[11775, 12371, 8170, 5, 8017, 7182, 19561, 7788, 12399, 6860, 8137, 32217, 10013, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[11775, 12371, 8170, 5, 8017, 7182, 19561, 7788, 12399, 6860, 8137, 32217, 10013, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:sentence: 부정 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "INFO:ratsnlp:sentence: 부정 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "INFO:ratsnlp:sentence: 부정 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "INFO:ratsnlp:tokens: ▁부정 ▁교도 소 ▁이야기 구 먼 ▁ .. 솔 직 히 ▁재 미는 ▁없다. . 평 점 ▁조정 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:tokens: ▁부정 ▁교도 소 ▁이야기 구 먼 ▁ .. 솔 직 히 ▁재 미는 ▁없다. . 평 점 ▁조정 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:tokens: ▁부정 ▁교도 소 ▁이야기 구 먼 ▁ .. 솔 직 히 ▁재 미는 ▁없다. . 평 점 ▁조정 </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s>\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[11775, 25365, 7824, 11120, 6919, 7514, 739, 9705, 7828, 8264, 8811, 9150, 16504, 22316, 389, 8656, 8191, 11840, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[11775, 25365, 7824, 11120, 6919, 7514, 739, 9705, 7828, 8264, 8811, 9150, 16504, 22316, 389, 8656, 8191, 11840, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[11775, 25365, 7824, 11120, 6919, 7514, 739, 9705, 7828, 8264, 8811, 9150, 16504, 22316, 389, 8656, 8191, 11840, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[11775, 25365, 7824, 11120, 6919, 7514, 739, 9705, 7828, 8264, 8811, 9150, 16504, 22316, 389, 8656, 8191, 11840, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[11775, 25365, 7824, 11120, 6919, 7514, 739, 9705, 7828, 8264, 8811, 9150, 16504, 22316, 389, 8656, 8191, 11840, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[11775, 25365, 7824, 11120, 6919, 7514, 739, 9705, 7828, 8264, 8811, 9150, 16504, 22316, 389, 8656, 8191, 11840, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:*** Example ***\n",
      "INFO:ratsnlp:sentence: 긍정 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "INFO:ratsnlp:sentence: 긍정 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "INFO:ratsnlp:sentence: 긍정 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "INFO:ratsnlp:tokens: ▁긍정 ▁사이 몬 페 그의 ▁익살 스런 ▁연기가 ▁돋보 였던 ▁영화 ! 스파 이 더 맨 에서 ▁늙 어 보 이기 만 ▁했던 ▁커 스 틴 ▁던 스트가 ▁너무나 도 ▁이 뻐\n",
      "INFO:ratsnlp:tokens: ▁긍정 ▁사이 몬 페 그의 ▁익살 스런 ▁연기가 ▁돋보 였던 ▁영화 ! 스파 이 더 맨 에서 ▁늙 어 보 이기 만 ▁했던 ▁커 스 틴 ▁던 스트가 ▁너무나 도 ▁이 뻐\n",
      "INFO:ratsnlp:tokens: ▁긍정 ▁사이 몬 페 그의 ▁익살 스런 ▁연기가 ▁돋보 였던 ▁영화 ! 스파 이 더 맨 에서 ▁늙 어 보 이기 만 ▁했던 ▁커 스 틴 ▁던 스트가 ▁너무나 도 ▁이 뻐\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[16420, 9435, 7543, 8643, 28657, 34499, 19912, 43245, 21778, 10463, 10584, 376, 19193, 8146, 7208, 7503, 9023, 17231, 8006, 7652, 11864, 7489, 13885, 10114, 7877, 8610, 13727, 38593, 25793, 7235, 9018, 7722], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[16420, 9435, 7543, 8643, 28657, 34499, 19912, 43245, 21778, 10463, 10584, 376, 19193, 8146, 7208, 7503, 9023, 17231, 8006, 7652, 11864, 7489, 13885, 10114, 7877, 8610, 13727, 38593, 25793, 7235, 9018, 7722])\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[16420, 9435, 7543, 8643, 28657, 34499, 19912, 43245, 21778, 10463, 10584, 376, 19193, 8146, 7208, 7503, 9023, 17231, 8006, 7652, 11864, 7489, 13885, 10114, 7877, 8610, 13727, 38593, 25793, 7235, 9018, 7722], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[16420, 9435, 7543, 8643, 28657, 34499, 19912, 43245, 21778, 10463, 10584, 376, 19193, 8146, 7208, 7503, 9023, 17231, 8006, 7652, 11864, 7489, 13885, 10114, 7877, 8610, 13727, 38593, 25793, 7235, 9018, 7722])\n",
      "INFO:ratsnlp:features: GenerationFeatures(input_ids=[16420, 9435, 7543, 8643, 28657, 34499, 19912, 43245, 21778, 10463, 10584, 376, 19193, 8146, 7208, 7503, 9023, 17231, 8006, 7652, 11864, 7489, 13885, 10114, 7877, 8610, 13727, 38593, 25793, 7235, 9018, 7722], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[16420, 9435, 7543, 8643, 28657, 34499, 19912, 43245, 21778, 10463, 10584, 376, 19193, 8146, 7208, 7503, 9023, 17231, 8006, 7652, 11864, 7489, 13885, 10114, 7877, 8610, 13727, 38593, 25793, 7235, 9018, 7722])\n",
      "INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n",
      "INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n",
      "INFO:ratsnlp:Saving features into cached file, it could take a lot of time...\n",
      "INFO:ratsnlp:Saving features into cached file ./korpora/nsmc/cached_train_PreTrainedTokenizerFast_32_nsmc_sentence-generation [took 3.171 s]\n",
      "INFO:ratsnlp:Saving features into cached file ./korpora/nsmc/cached_train_PreTrainedTokenizerFast_32_nsmc_sentence-generation [took 3.171 s]\n",
      "INFO:ratsnlp:Saving features into cached file ./korpora/nsmc/cached_train_PreTrainedTokenizerFast_32_nsmc_sentence-generation [took 3.171 s]\n"
     ]
    }
   ],
   "source": [
    "from ratsnlp.nlpbook.generation import NsmcCorpus, GenerationDataset\n",
    "from torch.utils.data import DataLoader, SequentialSampler, RandomSampler\n",
    "corpus = NsmcCorpus()\n",
    "train_dataset = GenerationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode='train',\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=RandomSampler(train_dataset, replacement=False),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ratsnlp:Loading features from cached file ./korpora/nsmc/cached_train_PreTrainedTokenizerFast_32_nsmc_sentence-generation [took 1.597 s]\n",
      "INFO:ratsnlp:Loading features from cached file ./korpora/nsmc/cached_train_PreTrainedTokenizerFast_32_nsmc_sentence-generation [took 1.597 s]\n",
      "INFO:ratsnlp:Loading features from cached file ./korpora/nsmc/cached_train_PreTrainedTokenizerFast_32_nsmc_sentence-generation [took 1.597 s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = GenerationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    args.pretrained_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsnlp.nlpbook.generation import GenerationTask\n",
    "task = GenerationTask(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = nlpbook.get_trainer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sbumseo/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:146: UserWarning: \n",
      "NVIDIA GeForce RTX 3060 Laptop GPU with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3060 Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "Missing logger folder: /home/sbumseo/git/kogpt2/src/script-writing/ckpt/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/sbumseo/.local/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py:380: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 125 M \n",
      "------------------------------------------\n",
      "125 M     Trainable params\n",
      "0         Non-trainable params\n",
      "125 M     Total params\n",
      "500.656   Total estimated model params size (MB)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/sbumseo/git/kogpt2/src/script-writing/SequenceGeneration.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbigpicture/home/sbumseo/git/kogpt2/src/script-writing/SequenceGeneration.ipynb#ch0000010vscode-remote?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbigpicture/home/sbumseo/git/kogpt2/src/script-writing/SequenceGeneration.ipynb#ch0000010vscode-remote?line=1'>2</a>\u001b[0m     task,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbigpicture/home/sbumseo/git/kogpt2/src/script-writing/SequenceGeneration.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m     train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbigpicture/home/sbumseo/git/kogpt2/src/script-writing/SequenceGeneration.ipynb#ch0000010vscode-remote?line=3'>4</a>\u001b[0m     val_dataloaders\u001b[39m=\u001b[39;49mval_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbigpicture/home/sbumseo/git/kogpt2/src/script-writing/SequenceGeneration.ipynb#ch0000010vscode-remote?line=4'>5</a>\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:768\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    750\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 768\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    769\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    770\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:721\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    720\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 721\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    722\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:809\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    805\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    806\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    807\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    808\u001b[0m )\n\u001b[0;32m--> 809\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    811\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    812\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1234\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1234\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1236\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1237\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1321\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1321\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1351\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1350\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/base.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_skip()\n\u001b[1;32m    197\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 199\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_run_start(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    201\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[1;32m    202\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:209\u001b[0m, in \u001b[0;36mFitLoop.on_run_start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39m\"\"\"Calls the ``on_train_start`` hook.\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m# reset train dataloader and val dataloader\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mreset_train_val_dataloaders(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mlightning_module)\n\u001b[1;32m    211\u001b[0m data_fetcher_cls \u001b[39m=\u001b[39m _select_data_fetcher(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer)\n\u001b[1;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher \u001b[39m=\u001b[39m data_fetcher_cls(prefetch_batches\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprefetch_batches)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1993\u001b[0m, in \u001b[0;36mTrainer.reset_train_val_dataloaders\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1985\u001b[0m \u001b[39m\"\"\"Resets train and val dataloaders if none are attached to the trainer.\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m \n\u001b[1;32m   1987\u001b[0m \u001b[39mThe val dataloader must be initialized before training loop starts, as the training loop\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1990\u001b[0m \u001b[39m    model: The ``LightningModule`` if called outside of the trainer scope.\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1993\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreset_train_dataloader(model\u001b[39m=\u001b[39;49mmodel)\n\u001b[1;32m   1994\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_dataloaders \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1995\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset_val_dataloader(model\u001b[39m=\u001b[39mmodel)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1889\u001b[0m, in \u001b[0;36mTrainer.reset_train_dataloader\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1884\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader \u001b[39m=\u001b[39m CombinedLoader(loaders, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mmultiple_trainloader_mode)\n\u001b[1;32m   1886\u001b[0m module \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatamodule\n\u001b[1;32m   1887\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_training_batches \u001b[39m=\u001b[39m (\n\u001b[1;32m   1888\u001b[0m     \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_dataloader)\n\u001b[0;32m-> 1889\u001b[0m     \u001b[39mif\u001b[39;00m has_len_all_ranks(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_dataloader, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstrategy, module)\n\u001b[1;32m   1890\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39minf\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1891\u001b[0m )\n\u001b[1;32m   1893\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlimit_train_batches, \u001b[39mint\u001b[39m):\n\u001b[1;32m   1894\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_training_batches \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_training_batches, \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlimit_train_batches))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:126\u001b[0m, in \u001b[0;36mhas_len_all_ranks\u001b[0;34m(dataloader, training_type, model)\u001b[0m\n\u001b[1;32m    123\u001b[0m local_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader)\n\u001b[1;32m    124\u001b[0m total_length \u001b[39m=\u001b[39m training_type\u001b[39m.\u001b[39mreduce(torch\u001b[39m.\u001b[39mtensor(local_length)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice), reduce_op\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msum\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 126\u001b[0m \u001b[39mif\u001b[39;00m total_length \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m:\n\u001b[1;32m    127\u001b[0m     rank_zero_warn(\n\u001b[1;32m    128\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal length of `\u001b[39m\u001b[39m{\u001b[39;00mdataloader\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` across ranks is zero.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m Please make sure this was your intention.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m total_length \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m local_length \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: no kernel image is available for execution on the device\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    task,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
