{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a19b76-30ec-47e6-8a6f-245ab4ad5d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-06 02:12:04.352683: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-06 02:12:06.462267: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-06 02:12:06.464026: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-06 02:12:06.464598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-06 02:12:06.465438: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-06 02:12:06.466434: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-06 02:12:06.467032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-06 02:12:06.467567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-06 02:12:09.165105: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-06 02:12:09.165683: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-06 02:12:09.166200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-06 02:12:09.166708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13263 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gluonnlp as nlp\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "import utils\n",
    "from glob import glob\n",
    "from gluonnlp.data import SentencepieceTokenizer # Use the tokenizer that SKT used to train KoGPT2\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b31ab9-a3c0-47ab-b76b-0c0b13ac63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(tf.keras.Model):\n",
    "    def __init__(self, dir_path):\n",
    "        super(GPT2Model, self).__init__()\n",
    "        self.gpt2 = TFGPT2LMHeadModel.from_pretrained(dir_path)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.gpt2(inputs)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba81301-2e0d-49ee-9b8d-166f33562a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ../gpt_ckpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL_PATH = '../gpt_ckpt'\n",
    "gpt_model = GPT2Model(BASE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b3fca5-7336-43a0-8811-def36f9bfaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10\n",
    "MAX_LEN = 30\n",
    "TOKENIZER_PATH = f'{BASE_MODEL_PATH}/gpt2_kor_tokenizer.spiece'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5dfa863-7983-4cfb-af5f-a4ac37d18d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
    "vocab = nlp.vocab.BERTVocab.from_sentencepiece(\n",
    "    TOKENIZER_PATH,\n",
    "    mask_token=None,\n",
    "    sep_token=None,\n",
    "    cls_token=None,\n",
    "    unknown_token='<unk>',\n",
    "    padding_token='<pad>',\n",
    "    bos_token='<s>',\n",
    "    eos_token='</s>'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "061c7e28-368f-48bb-990c-9d4cecf6a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-99999):\n",
    "    _logits = logits.numpy()\n",
    "    top_k = min(top_k, logits.shape[-1])\n",
    "    \n",
    "    if top_k > 0:\n",
    "        indices_to_remove = logits < tf.math.top_k(logits, top_k)[0][..., -1, None]\n",
    "        _logits[indices_to_remove] = filter_value\n",
    "    \n",
    "    if top_p > 0.0:\n",
    "        sorted_logits = tf.sort(logits, direction='DESCENDING')\n",
    "        sorted_indices = tf.argsort(logits, direction='DESCENDING')\n",
    "        cumulative_probs = tf.math.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "        \n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove = tf.concat([[False], sorted_indices_to_remove[..., :-1]], axis=0)\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove].numpy().tolist()\n",
    "        \n",
    "        _logits[indices_to_remove] = filter_value\n",
    "    return tf.constant([_logits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e886bd6-f901-43a7-8024-d636678ac063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sent(seed_word, model, max_step=100, greedy=False, top_k=0, top_p=0.):\n",
    "    sent = seed_word\n",
    "    toked = tokenizer(sent)\n",
    "    \n",
    "    for _ in range(max_step):\n",
    "        input_ids = tf.constant([vocab[vocab.bos_token],] + vocab[toked])[None, :]\n",
    "        outputs = model(input_ids)[:, -1, :]\n",
    "        \n",
    "        if greedy:\n",
    "            # Select the most probable word and convert it to text\n",
    "            gen = vocab.to_tokens(tf.argmax(outputs, axis=-1).numpy().tolist()[0])\n",
    "        else:\n",
    "            # Select word randomly based on probability distribution and convert it to text\n",
    "            output_logit = tf_top_k_top_p_filtering(outputs[0], top_k=top_k, top_p=top_p)\n",
    "            gen = vocab.to_tokens(tf.random.categorical(output_logit, 1).numpy().tolist()[0])[0]\n",
    "        \n",
    "        # Stop when eos token generated\n",
    "        if gen == vocab.eos_token:\n",
    "            break\n",
    "        \n",
    "        sent += gen.replace('▁', ' ')\n",
    "        toked = tokenizer(sent)\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc007793-93bf-437e-99d8-d94282f1db92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'오늘은 그녀와 함께                                                                                               '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('오늘', gpt_model, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3285871-3e35-4b77-ac4d-ef5cadba6c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때  네 주위에 있던 애들은 모두 없는 모양이 됐다.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', gpt_model, top_k=0, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2dd425c-e71e-4524-8a9b-3b99a2a83c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = './data/jjaltoon_scripts_130/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73cd4f2d-303f-4416-a067-5e1354c6ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "\n",
    "file_names = os.listdir(DATASET_PATH)\n",
    "for file_name in file_names:\n",
    "    file_path = os.path.join(DATASET_PATH, file_name)\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        sents += [s[:-1] for s in file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "926d9bf1-5229-4850-8852-8e544e922ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(data_path, pattern):\n",
    "    file_list = []\n",
    "    for dir, _, _ in os.walk(data_path):\n",
    "        file_list.extend(glob(os.path.join(dir, pattern)))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ed83304-83f2-44f3-8b00-724fc99a92a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58a2f479ac44d7e87c3cc412cd7624b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "input data files:   0%|          | 0/129 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "292139"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "file_list = get_file_list(DATASET_PATH, '*.csv')\n",
    "for file_path in tqdm(file_list, desc='input data files'):\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        speeches = []\n",
    "        for row in reader:\n",
    "            if row['type'] == 'speech':\n",
    "                speeches.append(row['speech'])\n",
    "    texts.append('\\n'.join(speeches))\n",
    "\n",
    "sents = ' '.join(texts)\n",
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fd00c79-583e-45f5-a206-fba6ad969019",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "for s in sents:\n",
    "    tokens = [vocab[vocab.bos_token],] + vocab[tokenizer(s)] + [vocab[vocab.eos_token],]\n",
    "    input_data.append(tokens[:-1])\n",
    "    output_data.append(tokens[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa9286f4-a1c7-43af-be11-30f598393fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = pad_sequences(input_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
    "output_data = pad_sequences(output_data, MAX_LEN, value=vocab[vocab.padding_token])\n",
    "\n",
    "input_data = np.array(input_data, dtype=np.int64)\n",
    "output_data = np.array(output_data, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dffe102b-706e-4d02-aa97-e0257c50ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3997b2c4-019b-49fa-b3d8-368b2204a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eb8d564-cbee-40da-9af0-85d65c74dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, vocab[vocab.padding_token]))\n",
    "    mask = tf.expand_dims(tf.cast(mask, dtype=pred.dtype), axis=-1)\n",
    "    \n",
    "    pred *= mask\n",
    "    acc = train_accuracy(real, pred)\n",
    "    \n",
    "    return tf.reduce_mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a603ecba-1cb9-4741-8302-72e8c7adc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model.compile(loss=loss_function,\n",
    "                  optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                  metrics=[accuracy_function])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc700bff-f6bd-4f6e-91f9-d82e07fa0a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16433/16433 [==============================] - 2786s 169ms/step - loss: 0.1404 - accuracy_function: 0.0602 - val_loss: 0.1376 - val_accuracy_function: 0.0610\n",
      "Epoch 2/10\n",
      "16433/16433 [==============================] - 2777s 169ms/step - loss: 0.1385 - accuracy_function: 0.0611 - val_loss: 0.1373 - val_accuracy_function: 0.0611\n",
      "Epoch 3/10\n",
      "16433/16433 [==============================] - 2713s 165ms/step - loss: 0.1383 - accuracy_function: 0.0611 - val_loss: 0.1371 - val_accuracy_function: 0.0611\n",
      "Epoch 4/10\n",
      "16433/16433 [==============================] - 2749s 167ms/step - loss: 0.1382 - accuracy_function: 0.0611 - val_loss: 0.1371 - val_accuracy_function: 0.0611\n",
      "Epoch 5/10\n",
      "16433/16433 [==============================] - 2767s 168ms/step - loss: 0.1381 - accuracy_function: 0.0612 - val_loss: 0.1371 - val_accuracy_function: 0.0612\n",
      "Epoch 6/10\n",
      "16433/16433 [==============================] - 2757s 168ms/step - loss: 0.1380 - accuracy_function: 0.0612 - val_loss: 0.1370 - val_accuracy_function: 0.0612\n",
      "Epoch 7/10\n",
      "16433/16433 [==============================] - 2798s 170ms/step - loss: 0.1380 - accuracy_function: 0.0612 - val_loss: 0.1370 - val_accuracy_function: 0.0612\n",
      "Epoch 8/10\n",
      "16433/16433 [==============================] - 2766s 168ms/step - loss: 0.1380 - accuracy_function: 0.0612 - val_loss: 0.1369 - val_accuracy_function: 0.0612\n",
      "Epoch 9/10\n",
      "16433/16433 [==============================] - 2711s 165ms/step - loss: 0.1380 - accuracy_function: 0.0612 - val_loss: 0.1371 - val_accuracy_function: 0.0612\n",
      "Epoch 10/10\n",
      "16433/16433 [==============================] - 2710s 165ms/step - loss: 0.1379 - accuracy_function: 0.0612 - val_loss: 0.1370 - val_accuracy_function: 0.0612\n"
     ]
    }
   ],
   "source": [
    "history = gpt_model.fit(input_data, output_data,\n",
    "                        batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "                        validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "386cad13-838c-480d-84b2-bf7bdedbf19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./data/out/tf2_gpt2_finetuned_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "DATA_OUT_PATH = './data/out'\n",
    "model_name = 'tf2_gpt2_finetuned_model'\n",
    "\n",
    "save_path = os.path.join(DATA_OUT_PATH, model_name)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "gpt_model.gpt2.save_pretrained(save_path)\n",
    "loaded_gpt_model = GPT2Model(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e13add9-ca97-4a6f-98d9-95eb62306646",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때!!!!!!!!!!!!!!!!!!!!!!!!!<s>!!!!!!!!!!!!!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', loaded_gpt_model, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6cf55cc-8534-4830-8916-9df7f0ef45e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이때해도장런시.!는!그요겠,인?도있었야게이?도오0주.도은대가(면부의대!.장지면수어!이도나수까!렇!..서스렇없죠무해에님임<unk>들으.(보도며마진니한!)?(가다은식데!(잖려!니끼는?!기!니?으'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sent('이때', loaded_gpt_model, top_k=0, top_p=0.75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
