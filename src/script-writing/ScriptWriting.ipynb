{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad09463-a9ee-4d34-af79-625a11be7002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelWithLMHead, PreTrainedTokenizerFast\n",
    "from fastai.text.all import *\n",
    "from tqdm.notebook import tqdm\n",
    "import fastai\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018a1a7b-280b-4c80-b1bf-9590999156bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "print(transformers.__version__)\n",
    "print(fastai.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd1388-7579-48d7-9c30-5f8ba99ba738",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PRETRAINED_MODEL = 'skt/kogpt2-base-v2'\n",
    "BATCH_SIZE = 8\n",
    "SEQ_LENGTH = 256\n",
    "MAX_LENGTH = 128\n",
    "DATASET_PATH = './dataset/jjaltoon_scripts_10_raw/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24297b2b-a8bf-4430-882f-14e24b83bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>',\n",
    "    mask_token='<mask>',\n",
    "    bos_token='<s>',\n",
    "    eos_token='</s>'\n",
    ")\n",
    "model = AutoModelWithLMHead.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9237cebc-1c3d-4567-9dad-82615931cd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('GPT-2 토크나이저 테스트. 안녕하세요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1038f2b8-3b8a-4169-ba08-b9490e204939",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '오늘의 메뉴는'\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = model.generate(\n",
    "    torch.tensor([input_ids]),\n",
    "    max_length=MAX_LENGTH,\n",
    "    repetition_penalty=2.0,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    use_cache=True\n",
    ")\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0599b3-1a90-432b-940e-717c6932ca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "\n",
    "file_names = os.listdir(DATASET_PATH)\n",
    "for file_name in tqdm(file_names, desc='input data files'):\n",
    "    file_path = os.path.join(DATASET_PATH, file_name)\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        file_content = file.read()\n",
    "    texts.append(' '.join(file_content.split()))\n",
    "\n",
    "data = ' '.join(texts)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d22017-6425-4c25-a65f-e4a097a0cffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "#data = re.sub('\\(계속\\).*?[●○]', '', data)\n",
    "#data = re.sub('[●○]', '', data)\n",
    "#len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075d6151-a5c1-40a4-9e5f-1846105848ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def encodes(self, x):\n",
    "        tokens = self.tokenizer.tokenize(x)\n",
    "        return tensor(self.tokenizer.convert_tokens_to_ids(tokens))\n",
    "    \n",
    "    def decodes(self, x):\n",
    "        return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c3d54-6095-492f-9bab-469bd3f82be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:int(len(data) * 0.9)]\n",
    "test_data = data[int(len(data) * 0.9):]\n",
    "splits = [[0], [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a8d5e9-f9fe-4ddb-8800-26b70cadf0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tls = TfmdLists([train_data, test_data], TransformersTokenizer(tokenizer), splits=splits, dl_type=LMDataLoader)\n",
    "dls = tls.dataloaders(bs=BATCH_SIZE, seq_len=SEQ_LENGTH)\n",
    "dls.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bf6902-1199-40f0-bedb-3ecc7272fc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropOutput(Callback):\n",
    "    def after_pred(self):\n",
    "        self.learn.pred = self.pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fddbc7-07ba-49b4-8de3-4c985add5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), cbs=[DropOutput], metrics=Perplexity()).to_fp16()\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d5242-438f-4ddf-a9c3-0e78b689be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.unfreeze()\n",
    "learn.fit_one_cycle(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60473e9-af39-451a-8f44-17c5db03a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(prompt):\n",
    "    prompt_ids = tokenizer.encode(prompt)\n",
    "    inp = tensor(prompt_ids)[None].cuda()\n",
    "    preds = learn.model.generate(\n",
    "        inp,\n",
    "        max_length=MAX_LENGTH,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        use_cache=True\n",
    "    )\n",
    "    return tokenizer.decode(preds[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3f7548-2dbf-4755-84d5-1cee9465387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sequence('인공지능')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
