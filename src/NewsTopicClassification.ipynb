{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 Model Experiment 1<br>\n",
    "Topic classification of news articles written in Korean\n",
    "- Data: [국립국어원 신문 말뭉치(v2)](https://corpus.korean.go.kr/) sampling data\n",
    "- Model: [SKT AI KoGPT2](https://github.com/SKT-AI/KoGPT2) fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [Seongbum Seo](https://github.com/Seongbuming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install transformers library\n",
    "%pip install -q git+https://github.com/huggingface/transformers.git\n",
    "# Install helper functions\n",
    "%pip install -q git+https://github.com/gmihaila/ml_things.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone base model\n",
    "!git clone https://github.com/SKT-AI/KoGPT2\n",
    "%pip install matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (set_seed, TrainingArguments, Trainer, GPT2Config, GPT2Tokenizer, AdamW, get_linear_schedule_with_warmup, GPT2ForSequenceClassification)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(123)\n",
    "\n",
    "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues\n",
    "# For small sequence length can try batch of 32 or higher\n",
    "batch_size = 32\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# If 'None' it will use maximum sequence of word piece tokens allowed by model\n",
    "max_length = 60\n",
    "\n",
    "# Look for GPU to use\n",
    "# Will use 'cpu' by default if no GPU found\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Name of the base model to use\n",
    "model_name_or_path = 'skt/kogpt2-base-v2'\n",
    "\n",
    "# Path of data to use for training and validation\n",
    "train_data_path = './data/sample_7000_3000/train'\n",
    "test_data_path = './data/sample_7000_3000/validation'\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert string labels to number ids\n",
    "labels_ids = {\n",
    "    'ITscience': 0,\n",
    "    'culture': 1,\n",
    "    'economy': 2,\n",
    "    'entertainment': 3,\n",
    "    'health': 4,\n",
    "    'life': 5,\n",
    "    'politic': 6,\n",
    "    'social': 7,\n",
    "    'sport': 8\n",
    "}\n",
    "\n",
    "# List of label names\n",
    "label_names = list(labels_ids.keys())\n",
    "\n",
    "# Number of labels\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, path, use_tokenizer):\n",
    "        # Check if path exists\n",
    "        if not os.path.isdir(path):\n",
    "            # Raise error if path is invalid\n",
    "            raise ValueError('Invalid `path` variable. Needs to be a directory.')\n",
    "        \n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Since the labels are defined by folders with data we loop through each label\n",
    "        for label in label_names:\n",
    "            sentiment_path = os.path.join(path, label)\n",
    "\n",
    "            # Get all files from path\n",
    "            files_names = os.listdir(sentiment_path)#[:10] # Sample for debugging\n",
    "            # Go through each file and read its content\n",
    "            for file_name in tqdm(files_names, desc=f'{label} files'):\n",
    "                file_path = os.path.join(sentiment_path, file_name)\n",
    "\n",
    "                # Read content\n",
    "                content = io.open(file_path, mode='r', encoding='utf-8').read()\n",
    "                # Fix any unicode issues\n",
    "                content = fix_text(content)\n",
    "                # Save content\n",
    "                self.texts.append(content)\n",
    "                # Save encode labels\n",
    "                self.labels.append(label)\n",
    "            \n",
    "        # Number of examples\n",
    "        self.n_examples = len(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        r'''When used `len` return the number of examples.\n",
    "        '''\n",
    "        \n",
    "        return self.n_examples\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        r'''Given an index return an example from the position.\n",
    "        \n",
    "        Arguments:\n",
    "            item(:obj:`int`):\n",
    "                Index position to pick an example to return.\n",
    "        \n",
    "        Returns:\n",
    "            :obj:`Dict[str, str]`: Dictionary of inputs that contain text and associated labels\n",
    "        '''\n",
    "\n",
    "        return {\n",
    "            'text': self.texts[item],\n",
    "            'label': self.labels[item]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gpt2ClassificationCollator(object):\n",
    "    r'''Data Collator used for GPT-2 in a classification rask.\n",
    "\n",
    "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that can go straight into a GPT-2 model.\n",
    "    \n",
    "    Arguments:\n",
    "        use_tokenizer(:obj:`transformers.tokenization_?`):\n",
    "            Transformer type tokenizer used to process raw text into numbers\n",
    "        labels_encoder(:obj:`dict`):\n",
    "            Dictionary to encode any labels names into numbers.\n",
    "            Keys map to labels names and Values map to number associated to those labels.\n",
    "        max_sequence_len(:obj:`int`, `optional`):\n",
    "            Value to indicate the maximum desired sequence to truncate or pad text sequences.\n",
    "            If no value is passed it will used maximum sequence size supported by the tokenizer and model.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
    "        # Tokenizer to be used inside the class\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        # Label encoder used inside the class\n",
    "        self.labels_encoder = labels_encoder\n",
    "    \n",
    "    def __call__(self, sequences):\n",
    "        r'''This function allowes the class object to be used as a function call.\n",
    "        Sine the PyTorch DataLoader needs a collator function, I can use this class as a function.\n",
    "\n",
    "        Arguments:\n",
    "            item(:obj:`list`):\n",
    "                List of texts and labels.\n",
    "        \n",
    "        Returns:\n",
    "            :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
    "            It holddes the statement `model(**Returned Dictionary)`.\n",
    "        '''\n",
    "\n",
    "        # Get all texts from sequences list\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        # Get all labels from sequences list\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        # Encode all labels using label encoder\n",
    "        labels = [self.labels_encoder[label] for label in labels]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with appropriate padding\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors='pt', padding=True, truncation=True, max_length=self.max_sequence_len)\n",
    "        # Update the inputs with the associated encoded labels as tensor\n",
    "        inputs.update({'labels': torch.tensor(labels)})\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer_, scheduler_, device_):\n",
    "    r'''Train PyTorch model on a single pass through the data loader.\n",
    "    \n",
    "    It will use the global variable `model` which is the transformer model loaded on `device_` that we want to train on.\n",
    "    \n",
    "    Arguments:\n",
    "        dataloader(:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "            Parsed data into batches of tensors.\n",
    "        optimizer_(:obj:`transformers.optimization.AdamW`):\n",
    "            Optimizer used for training.\n",
    "        scheduler_(:obj:`torch.optim.lr_scheduler.LambdaLR`):\n",
    "            PyTorch scheduler.\n",
    "        device_(:obj:`torch.device`):\n",
    "            Device used to load tensors before feeding to model.\n",
    "    \n",
    "    Returns:\n",
    "        :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss].\n",
    "    '''\n",
    "    \n",
    "    # Use global variable for model\n",
    "    global model\n",
    "\n",
    "    # Tracking variables\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    # Total loss for this epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        # Add original labels - use later for evaluation\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "        # Move batch to device\n",
    "        batch = {k: v.type(torch.long).to(device_) for k, v in batch.items()}\n",
    "\n",
    "        # Always clear any previously calculated gradients before performing a backward pass\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch)\n",
    "        # This will return the loss (rather than the model output)\n",
    "        # because we have provided the `labels`.\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull\n",
    "        # the loss value out of the tuple along with the logits\n",
    "        # We will use logits later to calculate training accuracy\n",
    "        loss, logits = outputs[:2]\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # caculate the average loss at the end\n",
    "        # The `loss` is a Tensor containing a single value\n",
    "        # The `.item()` function just returns the Python value from the tensor\n",
    "        total_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0\n",
    "        # This is to help prevent the exploding gradients problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        # The optimizer dictates the update rule - how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer_.step()\n",
    "        # Update the learning reate\n",
    "        scheduler_.step()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        # Convert these logits to list of predicted labels values\n",
    "        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "    # Calculate the average loss over the training data\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # Return all true labels and prediction for future evaluations\n",
    "    return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(dataloader, device_):\n",
    "    r'''Validation function to evaluate model performance on a separate set of data.\n",
    "    \n",
    "    This function will return the true and predicted labels so we can use later to evaluate the model's performance.\n",
    "    \n",
    "    Arguments:\n",
    "        dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "            Parsed data into batches of tensors.\n",
    "        device_(:obj:`torch.device`):\n",
    "            Device used to load tensors before feeding to model.\n",
    "    \n",
    "    Return:\n",
    "        :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted Labels, Train Average Loss]\n",
    "    '''\n",
    "    \n",
    "    # Use global variable for model\n",
    "    global model\n",
    "\n",
    "    # Tracking variables\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    # Total loss for this epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    # - the dropout layers differently during evaluation\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        # Add original labels\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "        # Move batch to device\n",
    "        batch = {k: v.type(torch.long).to(device_) for k, v in batch.items()}\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            # This will return the logits rather than the loss because we have not provided labels\n",
    "            # token_type_ids is the same as the 'segment ids', which differentiates sentence 1 and 2 in 2-sentence tasks\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "            # The call to `model` always returns a tuple, so we need to pull\n",
    "            # the loss value out of the tuple along with the logits later to calculate training accuracy\n",
    "            loss, logits = outputs[:2]\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            # Accumulate the training loss over all of the batches so that we can calculate the average loss at the end\n",
    "            # The `loss` is the Tensor containing a single value\n",
    "            # The `.item()` function just returns the Python value from the tensor\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Get predictions to list\n",
    "            predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
    "            # Update list\n",
    "            predictions_labels += predict_content\n",
    "    \n",
    "    # Calculate the average loss over the training data\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # Return all true labels and prediction for future evaluations\n",
    "    return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model configuration\n",
    "print('Loading configuration...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
    "\n",
    "# Get model's tokenizer\n",
    "print('Loading tokenizer...')\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    bos_token='</s>',\n",
    "    eos_token='</s>',\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>',\n",
    "    mask_token='<mask>'\n",
    ")\n",
    "# Default to left padding\n",
    "tokenizer.padding_side = 'left'\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get the actual model\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
    "\n",
    "# Resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "# Load model to define device\n",
    "model.to(device)\n",
    "print(f'Model loaded to `{device}`.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset_size = int(len(dataset) * 0.6)\n",
    "#valid_dataset_size = int(len(dataset) * 0.2)\n",
    "#test_dataset_size = len(dataset) - train_dataset_size - valid_dataset_size\n",
    "#train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_dataset_size, valid_dataset_size, test_dataset_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data collator to encode text and labels into numbers\n",
    "gpt2_classification_collator = Gpt2ClassificationCollator(\n",
    "    use_tokenizer=tokenizer,\n",
    "    labels_encoder=labels_ids,\n",
    "    max_sequence_len=max_length\n",
    ")\n",
    "\n",
    "print('Dealing with train...')\n",
    "# Create pytorch dataset\n",
    "train_dataset = NewsDataset(path=train_data_path, use_tokenizer=tokenizer)\n",
    "print(f'Created `train_dataset` with {len(train_dataset)} examples.')\n",
    "\n",
    "# Move pytorch dataset into dataloder\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_classification_collator)\n",
    "print(f'Created `train_dataloader` with {len(train_dataloader)} batches.')\n",
    "print()\n",
    "\n",
    "print('Dealing with validation...')\n",
    "# Create pytorch dataset\n",
    "valid_dataset = NewsDataset(path=test_data_path, use_tokenizer=tokenizer)\n",
    "print(f'Created `valid_dataset` with {len(valid_dataset)} examples.')\n",
    "\n",
    "# Move pytorch dataset into dataloader\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classification_collator)\n",
    "print(f'Created `valid_dataloader` with {len(valid_dataloader)} batches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdamW is a class from the huggingface library (as opposed to pytorch)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8) # by default lr is 5e-5 and eps is 1e-8\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs\n",
    "# `train_dataloader` contains batches data so `len(train_dataloader)` gives us the number of batches\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "# Create the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them\n",
    "all_loss = {'train_loss': [], 'val_loss': []}\n",
    "all_acc = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "# Loop through each epoch\n",
    "print('Epoch')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print()\n",
    "    print('Training on batches...')\n",
    "    # Perform one full pass over the training set\n",
    "    train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
    "    train_acc = accuracy_score(train_labels, train_predict)\n",
    "\n",
    "    print('Validation on batches...')\n",
    "    # Get prediction form model on validation data\n",
    "    valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
    "    val_acc = accuracy_score(valid_labels, valid_predict)\n",
    "\n",
    "    # Print loss and accuracy values to see how training evolves\n",
    "    print('  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - val_acc: %.5f' % (train_loss, val_loss, train_acc, val_acc))\n",
    "    print()\n",
    "\n",
    "    # Store the loss value for plotting the learning curve\n",
    "    all_loss['train_loss'].append(train_loss)\n",
    "    all_loss['val_loss'].append(val_loss)\n",
    "    all_acc['train_acc'].append(train_acc)\n",
    "    all_acc['val_acc'].append(val_acc)\n",
    "\n",
    "# Plot loss curves\n",
    "plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'], use_title='Loss')\n",
    "# Plot accuracy curves\n",
    "plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'], use_title='Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction form model on validation data\n",
    "# This is where you should use your test data\n",
    "true_labels, predictions_labels, avg_epoch_loss = validation(valid_dataloader, device)\n",
    "\n",
    "# Create the evaluation report\n",
    "evaluation_report = classification_report(true_labels, predictions_labels, labels=list(labels_ids.values()), target_names=label_names)\n",
    "# Show the evaluation report\n",
    "print(evaluation_report)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_true=true_labels, y_pred=predictions_labels, classes=label_names, normalize=True, magnify=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "5eb98d62cb1847a13fed7eb12c2a9611413ceb2c75685db78a43742deebc44b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
