{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff8777a-aae5-4c31-b4f0-e27b0257f977",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Script Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8808964-fd58-40d4-8f30-ff753dceb1e4",
   "metadata": {},
   "source": [
    "GPT-2 Model Experiment 2<br>\n",
    "Script writing in Korean\n",
    "- Data: [짤툰](https://www.youtube.com/c/%EC%A7%A4%ED%88%B01) script data\n",
    "- Model: [SKT AI KoGPT2](https://github.com/SKT-AI/KoGPT2) fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d4e8c-ae61-47dd-bace-c6cd2ef924eb",
   "metadata": {},
   "source": [
    "Author: [Seongbum Seo](https://github.com/Seongbuming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d233ee4-cccb-407f-b735-9704e72780ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5e40f-4428-4a00-9bb4-6708479e0a43",
   "metadata": {},
   "source": [
    "## Background Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1004042-ebde-4552-a82b-a65dd75bba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install transformers library\n",
    "%pip install -q git+https://github.com/huggingface/transformers.git\n",
    "# Install helper functions\n",
    "%pip install -q git+https://github.com/gmihaila/ml_things.git\n",
    "%pip install -q fastai==2.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd515eb-a3e5-4306-a9c2-d1b9d24c543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'KoGPT2' already exists and is not an empty directory.\n",
      "Collecting matplotlib==3.1.3\n",
      "  Using cached matplotlib-3.1.3-cp38-cp38-manylinux1_x86_64.whl (13.1 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (1.4.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (1.22.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.14.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.2\n",
      "    Uninstalling matplotlib-3.5.2:\n",
      "      Successfully uninstalled matplotlib-3.5.2\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "ml-things 0.0.1 requires matplotlib>=3.4.0, but you'll have matplotlib 3.1.3 which is incompatible.\u001b[0m\n",
      "Successfully installed matplotlib-3.1.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Clone base model\n",
    "!git clone https://github.com/SKT-AI/KoGPT2\n",
    "%pip install matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523830f-e75e-4a24-8030-597c338949c8",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23aecab5-be93-42b5-9c1e-054ac4ea0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import fastai\n",
    "import re\n",
    "from typing import Optional\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from ml_things import fix_text\n",
    "from transformers import AutoModelWithLMHead, PreTrainedTokenizerFast, GPT2Config, AdamW, get_linear_schedule_with_warmup\n",
    "from fastai.text.all import *\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(123)\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues\n",
    "# For small sequence length can try batch of 32 or higher\n",
    "batch_size = 8\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# If 'None' it will use maximum sequence of word piece tokens allowed by model\n",
    "max_length = 256\n",
    "\n",
    "# Look for GPU to use\n",
    "# Will use 'cpu' by default if no GPU found\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Name of the base model to use\n",
    "model_name_or_path = 'skt/kogpt2-base-v2'\n",
    "\n",
    "# Path of data to use for training\n",
    "data_path = './dataset/jjaltoon_scripts'\n",
    "\n",
    "kogpt2_config = {\n",
    "    'initializer_range': 0.02,\n",
    "    'layer_norm_epsilon': 1e-05,\n",
    "    'n_ctx': 1024,\n",
    "    'n_embd': 768,\n",
    "    'n_head': 12,\n",
    "    'n_layer': 12,\n",
    "    'n_positions': 1024,\n",
    "    'vocab_size': 50000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9677d75-6a3b-42b9-a51e-521d976105d7",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6871e15-ea34-4cac-92f4-cd4530479938",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScriptDataset(Dataset):\n",
    "    def __init__(self, path, use_tokenizer):\n",
    "        # Check if path exists\n",
    "        if not os.path.isdir(path):\n",
    "            # Raise error if path is invalid\n",
    "            raise ValueError('Invalid `path` variable. Needs to be a directory.')\n",
    "        \n",
    "        self.examples = []\n",
    "        \n",
    "        # Get all files from path\n",
    "        files_names = os.listdir(path)\n",
    "        # Go through each file and read its content\n",
    "        for file_name in tqdm(files_names, desc=f'script files'):\n",
    "            file_path = os.path.join(path, file_name)\n",
    "            \n",
    "            # Read content\n",
    "            content = io.open(file_path, mode='r', encoding='utf-8').read()\n",
    "            # Fix any unicode issues\n",
    "            content = fix_text(content)\n",
    "            # Save content\n",
    "            self.examples.append(content)\n",
    "        \n",
    "        # Number of examples\n",
    "        self.n_examples = len(self.examples)\n",
    "    \n",
    "    def __len__(self):\n",
    "        r'''When used `len` return the number of examples.\n",
    "        '''\n",
    "        \n",
    "        return self.n_examples\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        r'''Given an index return an example from the position.\n",
    "        \n",
    "        Arguments:\n",
    "            item(:obj:`int`):\n",
    "                Index position to pick an example to return.\n",
    "        \n",
    "        Returns:\n",
    "            :obj:`str`: Script of the index position.\n",
    "        '''\n",
    "        \n",
    "        return self.examples[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1bbc42-23c1-44b9-9a44-b1bf24ecd1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gpt2ScriptWritingCollator(object):\n",
    "    r'''Data Collator used for GPT-2 in a script writing task.\n",
    "    \n",
    "    It uses a given tokenizer and its encoder to convert any text to numbers that can go straight into a GPT-2 model.\n",
    "    \n",
    "    Arguments:\n",
    "        use_tokenizer(:obj:`transformers.tokenization_?`):\n",
    "            Transformer type tokenizer used to process raw text into numbers\n",
    "        max_sequence_len(:obj:`int`, `optional`):\n",
    "            Value to indicate the maximum desired sequence to truncate or pad text sequences.\n",
    "            If no value is passed it will used maximum sequence size supported by the tokenizer and model.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, use_tokenizer, max_sequence_len=None):\n",
    "        # Tokenizer to be used inside the class\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "    \n",
    "    def __call__(self, sequences):\n",
    "        r'''This function allowes the class object to be used as a function call.\n",
    "        \n",
    "        Since the PyTorch DataLoader needs a collator function, can use this class as a function.\n",
    "        \n",
    "        Arguments:\n",
    "            item(:obj:`list`):\n",
    "                List of texts.\n",
    "        \n",
    "        Returns:\n",
    "            :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
    "            It holds the statement `model(**Returned Dictionary)`.\n",
    "        '''\n",
    "        \n",
    "        # Get all texts from sequences list\n",
    "        text = [sequence['text'] for sequence in sequences]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with appropriate padding\n",
    "        inputs = self.use_tokenizer(text=text, return_tensors='pt', padding=True, truncate=True, max_length=self.max_sequence_len)\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc431383-4976-4557-8724-4cd2cb31a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer_, scheduler_, device_):\n",
    "    r'''Train PyTorch model on a single pass through the data loader.\n",
    "    \n",
    "    It will use the global variable `model` which is the transformer model loaded on `device_` that we want to train on.\n",
    "    \n",
    "    Arguments:\n",
    "        dataloader(:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "            Parsed data into batches of tensors.\n",
    "        optimizer_(:obj:`transformers.optimization.AdamW`):\n",
    "            Optimizer used for training.\n",
    "        scheduler_(:obj:`torch.optim.lr_scheduler.LambdaLR`):\n",
    "            PyTorch scheduler.\n",
    "        device_(:obj:`torch.device`):\n",
    "            Device used to load tensors before feeding to model.\n",
    "        \n",
    "        Returns:\n",
    "            :obj:`List[\n",
    "    '''\n",
    "    \n",
    "    #learn = Learner(dataloader, loss_func=CrossEntropyLossFlat(), cbs=[Dropout], metrics=Perplexity()).to_fp16()\n",
    "    #lr = learn.lr_find()\n",
    "    #print(f'learning rate: {lr}')\n",
    "    #learn.fine_tune(epochs)\n",
    "    \n",
    "    # Use global variable for model\n",
    "    global model\n",
    "    \n",
    "    # Total loss for this epoch\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Put the model into training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Bor deach batch of training data\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        # Always clear any previously calculated gradients before performing a backward pass\n",
    "        # If use `optimizer.zero_grad()` it will call zero_grad() for all parameters registered in the optimizer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Since it is composed of a list of tensors, it converts the list through the stack\n",
    "        data = torch.stack(batch)\n",
    "        data = data.transpose(1, 0)\n",
    "        # Move batch to device\n",
    "        data = data.to(device_)\n",
    "        #model = model.to(device_)\n",
    "        \n",
    "        # Perform a forward pass\n",
    "        outputs = model(data, labels=data)\n",
    "        \n",
    "        # The call to `model` always returns a tuple, so we need to pull\n",
    "        # the loss value out of the tuple along with the logits\n",
    "        loss, logits = outputs[:2]\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # caculate the average loss at the end\n",
    "        # The `loss` is a Tensor containing a single value\n",
    "        # The `.item()` function just returns the Python value from the tensor\n",
    "        total_loss += loss.item()\n",
    "        # Perform a backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0\n",
    "        # This is to help prevent the exploding radients problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        # The optimizer dictates the update rule - how the parameters are\n",
    "        # modified based on their gradients, the learning rate, etc.\n",
    "        optimizer_.step()\n",
    "        # Update the learning rate\n",
    "        scheduler_.step()\n",
    "    \n",
    "    # Calculate the average loss over the training data\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "        \n",
    "    # Reutrn average loss at this epoch\n",
    "    return avg_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d4561-4ebe-4c88-b97b-e83dde183169",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d55e6a0-d477-4ec1-94b0-26d41c23b218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration...\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/auto/modeling_auto.py:969: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded to `cuda`.\n"
     ]
    }
   ],
   "source": [
    "# Get model configuration\n",
    "print('Loading configuration...')\n",
    "model_config = GPT2Config.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name_or_path\n",
    ")\n",
    "\n",
    "# Get model's tokenizer\n",
    "print('Loading tokenizer...')\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    bos_token='</s>',\n",
    "    eos_token='</s>',\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>',\n",
    "    mask_token='<mask>'\n",
    ")\n",
    "# Default to left padding\n",
    "tokenizer.padding_side = 'left'\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get the actual model\n",
    "print('Loading model...')\n",
    "model = AutoModelWithLMHead.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name_or_path,\n",
    "    config=model_config\n",
    ")\n",
    "\n",
    "# Resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "# Load model to define device\n",
    "model.to(device)\n",
    "print(f'Model loaded to `{device}`.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53eb16aa-5acb-4f8d-91ef-88e0f89f7a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1819c96ce94edd80f80381c2d1f51f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "script files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `dataset` with 10 examples.\n",
      "Created `train_dataloader` with 2 batches.\n"
     ]
    }
   ],
   "source": [
    "# Create data collator to encode texts into numbers\n",
    "gpt2_script_writing_collator = Gpt2ScriptWritingCollator(\n",
    "    use_tokenizer=tokenizer,\n",
    "    max_sequence_len=max_length\n",
    ")\n",
    "\n",
    "# Create PyTorch dataset\n",
    "print('Dealing with train...')\n",
    "train_dataset = ScriptDataset(path=data_path, use_tokenizer=tokenizer)\n",
    "print(f'Created `dataset` with {len(dataset)} examples.')\n",
    "\n",
    "# Move PyTorch dataset into dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_script_writing_collator)\n",
    "print(f'Created `train_dataloader` with {len(train_dataloader)} batches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0e8a9-edf8-42b9-bd91-2cfc78a60d3e",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fadd18fb-afcd-4b55-a768-59e09a4c3d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e128038b66294098a204bc1fff126c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training on batches...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7f372d6465478faef74c7e3455975a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining on batches...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  train_loss: \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m train_loss)\n\u001b[1;32m     18\u001b[0m all_loss[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, optimizer_, scheduler_, device_)\u001b[0m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Since it is composed of a list of tensors, it converts the list through the stack\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Move batch to device\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got str"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "all_loss = {'train_loss': [], 'val_loss': []}\n",
    "all_acc = {'train_acc': [], 'val_acc': []}\n",
    "\n",
    "print('Epoch')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print()\n",
    "    print('Training on batches...')\n",
    "    \n",
    "    train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
    "    \n",
    "    print('  train_loss: %.5f' % train_loss)\n",
    "    \n",
    "    all_loss['train_loss'].append(train_loss)\n",
    "        \n",
    "    if epoch > 0 and epoch % 10 == 0:\n",
    "        # Move model to CPU and generate sentences to evaluate writing performance in the current epoch\n",
    "        #sentence = generate_sentences(model.to('cpu'), tokenizer, vocab, sent='내일', text_size=100, temperature=0.7, top_p=0.8, top_k=40)\n",
    "        #sentence = sentence.replace('<unused0>', '\\n')\n",
    "        #sentence = auto_enter(sentence)\n",
    "        inputs = tokenizer('내일', return_tensors='pt', padding=True, truncate=True, max_length=max_length)\n",
    "        generation = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n",
    "        \n",
    "        print(generation[:2])\n",
    "\n",
    "        # Visualize the generated sentences through tensorboard\n",
    "        #summary.add_text('Text', sentence, count)\n",
    "    \n",
    "    print()\n",
    "\n",
    "    count += 1\n",
    "\n",
    "\n",
    "# Plot loss curves\n",
    "plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'], use_title='Loss')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc70e84e-9f04-4aa1-9a77-3cd288a31c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
