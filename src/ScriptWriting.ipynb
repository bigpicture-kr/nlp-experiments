{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ff8777a-aae5-4c31-b4f0-e27b0257f977",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Script Writing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8808964-fd58-40d4-8f30-ff753dceb1e4",
   "metadata": {},
   "source": [
    "GPT-2 Model Experiment 2<br>\n",
    "Script writing in Korean\n",
    "- Data: [짤툰](https://www.youtube.com/c/%EC%A7%A4%ED%88%B01) script data\n",
    "- Model: [SKT AI KoGPT2](https://github.com/SKT-AI/KoGPT2) fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d4e8c-ae61-47dd-bace-c6cd2ef924eb",
   "metadata": {},
   "source": [
    "Author: [Seongbum Seo](https://github.com/Seongbuming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d233ee4-cccb-407f-b735-9704e72780ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5e40f-4428-4a00-9bb4-6708479e0a43",
   "metadata": {},
   "source": [
    "## Background Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1004042-ebde-4552-a82b-a65dd75bba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install transformers library\n",
    "%pip install -q git+https://github.com/huggingface/transformers.git\n",
    "# Install helper functions\n",
    "%pip install -q git+https://github.com/gmihaila/ml_things.git\n",
    "%pip install -q fastai==2.2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd515eb-a3e5-4306-a9c2-d1b9d24c543e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'KoGPT2' already exists and is not an empty directory.\n",
      "Collecting matplotlib==3.1.3\n",
      "  Using cached matplotlib-3.1.3-cp38-cp38-manylinux1_x86_64.whl (13.1 MB)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (1.4.3)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (2.8.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib==3.1.3) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.1->matplotlib==3.1.3) (1.14.0)\n",
      "Installing collected packages: matplotlib\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.5.2\n",
      "    Uninstalling matplotlib-3.5.2:\n",
      "      Successfully uninstalled matplotlib-3.5.2\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "ml-things 0.0.1 requires matplotlib>=3.4.0, but you'll have matplotlib 3.1.3 which is incompatible.\u001b[0m\n",
      "Successfully installed matplotlib-3.1.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Clone base model\n",
    "!git clone https://github.com/SKT-AI/KoGPT2\n",
    "%pip install matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d523830f-e75e-4a24-8030-597c338949c8",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23aecab5-be93-42b5-9c1e-054ac4ea0a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import fastai\n",
    "import re\n",
    "from typing import Optional\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from ml_things import fix_text\n",
    "from transformers import AutoModelWithLMHead, PreTrainedTokenizerFast, GPT2Config\n",
    "from fastai.text.all import *\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 10\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues\n",
    "# For small sequence length can try batch of 32 or higher\n",
    "batch_size = 8\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# If 'None' it will use maximum sequence of word piece tokens allowed by model\n",
    "max_length = 256\n",
    "\n",
    "# Look for GPU to use\n",
    "# Will use 'cpu' by default if no GPU found\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Name of the base model to use\n",
    "model_name_or_path = 'skt/kogpt2-base-v2'\n",
    "\n",
    "# Path of data to use for training\n",
    "train_data_path = './dataset/jjaltoon_scripts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9677d75-6a3b-42b9-a51e-521d976105d7",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6871e15-ea34-4cac-92f4-cd4530479938",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScriptDataset(Dataset):\n",
    "    def __init__(self, path, use_tokenizer):\n",
    "        # Check if path exists\n",
    "        if not os.path.isdir(path):\n",
    "            # Raise error if path is invalid\n",
    "            raise ValueError('Invalid `path` variable. Needs to be a directory.')\n",
    "        \n",
    "        self.examples = []\n",
    "        \n",
    "        # Get all files from path\n",
    "        files_names = os.listdir(path)\n",
    "        # Go through each file and read its content\n",
    "        for file_name in tqdm(files_names, desc=f'script files'):\n",
    "            file_path = os.path.join(path, file_name)\n",
    "            \n",
    "            # Read content\n",
    "            content = io.open(file_path, mode='r', encoding='utf-8').read()\n",
    "            # Fix any unicode issues\n",
    "            content = fix_text(content)\n",
    "            # Save content\n",
    "            self.examples.append(content)\n",
    "        \n",
    "        # Number of examples\n",
    "        self.n_examples = len(self.examples)\n",
    "    \n",
    "    def __len__(self):\n",
    "        r'''When used `len` return the number of examples.\n",
    "        '''\n",
    "        \n",
    "        return self.n_examples\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        r'''Given an index return an example from the position.\n",
    "        \n",
    "        Arguments:\n",
    "            item(:obj:`int`):\n",
    "                Index position to pick an example to return.\n",
    "        \n",
    "        Returns:\n",
    "            :obj:`str`: Script of the index position.\n",
    "        '''\n",
    "        \n",
    "        return self.examples[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fafd83-ecd0-493f-a3c5-edde74a2565e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersTokenizer(Transform):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def encode(self, x):\n",
    "        tokens = self.tokenizer.tokenize(x)\n",
    "        return tensor(self.tokenizer.convert_tokens_to_ids(tokens))\n",
    "    \n",
    "    def decodes(self, x):\n",
    "        return TitledStr(self.tokenizer.decode(x.cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b1bbc42-23c1-44b9-9a44-b1bf24ecd1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gpt2ScriptWritingCollator(object):\n",
    "    r'''Data Collator used for GPT-2 in a script writing task.\n",
    "    \n",
    "    It uses a given tokenizer and its encoder to convert any text to numbers that can go straight into a GPT-2 model.\n",
    "    \n",
    "    Arguments:\n",
    "        use_tokenizer(:obj:`transformers.tokenization_?`):\n",
    "            Transformer type tokenizer used to process raw text into numbers\n",
    "        max_sequence_len(:obj:`int`, `optional`):\n",
    "            Value to indicate the maximum desired sequence to truncate or pad text sequences.\n",
    "            If no value is passed it will used maximum sequence size supported by the tokenizer and model.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, use_tokenizer, max_sequence_len=None):\n",
    "        # Tokenizer to be used inside the class\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "    \n",
    "    def __call__(self, sequences):\n",
    "        r'''This function allowes the class object to be used as a function call.\n",
    "        \n",
    "        Since the PyTorch DataLoader needs a collator function, can use this class as a function.\n",
    "        \n",
    "        Arguments:\n",
    "            item(:obj:`list`):\n",
    "                List of texts.\n",
    "        \n",
    "        Returns:\n",
    "            :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
    "            It holds the statement `model(**Returned Dictionary)`.\n",
    "        '''\n",
    "        \n",
    "        # Get all texts from sequences list\n",
    "        text = [sequence['text'] for sequence in sequences]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with appropriate padding\n",
    "        inputs = self.use_tokenizer(text=text, return_tensors='pt', padding=True, truncate=True, max_length=self.max_sequence_len)\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c2312f-f240-4500-bede-6cccfa3d0756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Callback):\n",
    "    def after_pred(self):\n",
    "        self.learn.pred = self.pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc431383-4976-4557-8724-4cd2cb31a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer_, scheduler_, device_):\n",
    "    r'''Train PyTorch model on a single pass through the data loader.\n",
    "    \n",
    "    It will use the global variable `model` which is the transformer model loaded on `device_` that we want to train on.\n",
    "    \n",
    "    Arguments:\n",
    "        dataloader(:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "            Parsed data into batches of tensors.\n",
    "        optimizer_(:obj:`transformers.optimization.AdamW`):\n",
    "            Optimizer used for training.\n",
    "        scheduler_(:obj:`torch.optim.lr_scheduler.LambdaLR`):\n",
    "            PyTorch scheduler.\n",
    "        device_(:obj:`torch.device`):\n",
    "            Device used to load tensors before feeding to model.\n",
    "        \n",
    "        Returns:\n",
    "            :obj:`List[\n",
    "    '''\n",
    "    \n",
    "    # Use global variable for model\n",
    "    learn = Learner(dataloader, loss_func=CrossEntropyLossFlat(), cbs=[Dropout], metrics=Perplexity()).to_fp16()\n",
    "    lr = learn.lr_find()\n",
    "    print(f'learning rate: {lr}')\n",
    "    learn.fine_tune(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4d4561-4ebe-4c88-b97b-e83dde183169",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d55e6a0-d477-4ec1-94b0-26d41c23b218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configuration...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ee2301e6dd44cdac64a82a11f83f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa99b2b6ea84f368693996480b5a529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/auto/modeling_auto.py:969: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caffd41b1fff4277a5f2d3de763284ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/490M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded to `cuda`.\n"
     ]
    }
   ],
   "source": [
    "# Get model configuration\n",
    "print('Loading configuration...')\n",
    "model_config = GPT2Config.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name_or_path\n",
    ")\n",
    "\n",
    "# Get model's tokenizer\n",
    "print('Loading tokenizer...')\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    bos_token='</s>',\n",
    "    eos_token='</s>',\n",
    "    unk_token='<unk>',\n",
    "    pad_token='<pad>',\n",
    "    mask_token='<mask>'\n",
    ")\n",
    "# Default to left padding\n",
    "tokenizer.padding_side = 'left'\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Get the actual model\n",
    "print('Loading model...')\n",
    "model = AutoModelWithLMHead.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name_or_path,\n",
    "    config=model_config\n",
    ")\n",
    "\n",
    "# Resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "# Load model to define device\n",
    "model.to(device)\n",
    "print(f'Model loaded to `{device}`.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb16aa-5acb-4f8d-91ef-88e0f89f7a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dealing with train...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "script files:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `train_dataset` with 10 examples.\n",
      "Created `train_dataloader` with 2 batches.\n"
     ]
    }
   ],
   "source": [
    "# Create data collator to encode texts into numbers\n",
    "gpt2_script_writing_collator = Gpt2ScriptWritingCollator(\n",
    "    use_tokenizer=tokenizer,\n",
    "    max_sequence_len=max_length\n",
    ")\n",
    "\n",
    "# Create PyTorch dataset\n",
    "print('Dealing with train...')\n",
    "train_dataset = ScriptDataset(path=train_data_path, use_tokenizer=tokenizer)\n",
    "print(f'Created `train_dataset` with {len(train_dataset)} examples.')\n",
    "\n",
    "# Move PyTorch dataset into dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=gpt2_script_writing_collator)\n",
    "print(f'Created `train_dataloader` with {len(train_dataloader)} batches.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f21e57-ef67-4231-bf7e-782fbb23bfe4",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a39a06-c97f-4139-938a-dbc6af7d2759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1f0e8a9-edf8-42b9-bd91-2cfc78a60d3e",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadd18fb-afcd-4b55-a768-59e09a4c3d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
